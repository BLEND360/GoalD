{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwgR8S_y0QUr"
      },
      "source": [
        "# **GD AI - Building Transformers The GD Way!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aijOQtLg0QIy"
      },
      "source": [
        "\n",
        "\n",
        "> In this activity, each of the chosen GDs, will fill out their designated code sections to win some cool GDPoints. Top 3 GDs to finish early will receive 20,15 and 10 GDPoints respectively ;) Happy Coding!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIS1ju4q0OoX",
        "outputId": "aa130c90-347a-46ce-83a1-8cb7c8622de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3cBIgbdMzpeg"
      },
      "outputs": [],
      "source": [
        "#Import relevant libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "torchtext.disable_torchtext_deprecation_warning() #Disable warnings\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2xwxXf74PHl"
      },
      "source": [
        "\n",
        "\n",
        "> Initialize the following modules within the Transformer class:\n",
        "\n",
        "*   Positional Encoder to add positional information to input tokens\n",
        "*   Embedding Layer to convert tokens to vectors\n",
        "*   Transformer Module (Feed-Forward Neural Network) to process these vectors\n",
        "*   Linear Decoder to convert processed vectors back to token probabilities\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v9KwyfGU4GFT"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    ### GD On Duty: ###\n",
        "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, dropout=0.5):\n",
        "    # This function will initialize the instance variables of the Transformer model pipeline\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout), nlayers)\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "        self.init_weights()\n",
        "\n",
        "############################################################\n",
        "# Key QnA:\n",
        "# 1) Why do we need to uniformly initialize the weights of the embedding encoding layer in a transformer? Ans) To prevent vanishing/exploding gradients\n",
        "# 2) Why do we need to set the biases of the decoder layer to zero? Ans) To provide a neutral starting point, allowing the model to learn appropriate biases from the data without initial bias influence.\n",
        "############################################################\n",
        "\n",
        "    def init_weights(self):\n",
        "    # This function will initialize weights of the model's components to improve training stability and performance\n",
        "        initrange = 0.1 # Sets a range value for initializing the weights.\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange) # Initializes the weights of the embedding layer self.encoder with values uniformly sampled from the range [−0.1,0.1].\n",
        "        self.decoder.bias.data.zero_() # Sets the biases of the linear layer self.decoder to zero.\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange) # Initializes the weights of the linear layer self.decoder with values uniformly sampled from the range [−0.1,0.1].\n",
        "    # These initializations help in starting the training process with small, random values for weights and zero biases, which is generally a good practice to facilitate effective learning.\n",
        "\n",
        "    ### GD On Duty: ###\n",
        "    def forward(self, src, src_mask):\n",
        "    # This function identifies a forward pass of the data through the Transformer pipeline\n",
        "        src = self.encoder(src) * math.sqrt(d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "    def save_model(self, path):\n",
        "      torch.save(self.state_dict(), path)\n",
        "      print(f\"Model saved to {path}\")\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "    # This mask ensures that each position in a sequence attends only to positions before it, not to positions that come after it. It's like giving the model a hint to focus only on past\n",
        "    # information when processing each token, helping it learn dependencies without \"cheating\" by looking at future tokens during training.\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA24rYghSaY9"
      },
      "source": [
        "\n",
        "> Initialize the following modules within the PositionalEncoding class:\n",
        "\n",
        "*   Dropout Layer - Helps prevent overfitting\n",
        "*   Positional Encoding Matrix - To provide information about the position of each token in the sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NHnupczG4GC5"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    ### GD On Duty: ###\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "    # This function adds positional information to token embeddings in a sequence, which is crucial for the Transformer model to capture the order of tokens.\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout) # Define a drop out layer with the specified probability\n",
        "\n",
        "        #Create a positional encoding matrix - This matrix will encode the sequence order within the embedded tokens\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "    # This function represents a forward pass on Positional Encoding within the Transformer model pipeline\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XYc3ShjS0xU"
      },
      "source": [
        "> The TextDataset class will process a given text into \"input-target\" pairs of token sqequences where:\n",
        "*    The input sequence includes all tokens except the last one\n",
        "*    The output sequence includes all tokens except the first one (shifted by one position)\n",
        "\n",
        "The purpose of this exercise is to have the model learn to predict the next token in the sequence based on the preceding tokens, for successful sequence generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TCHA2E3A4GAs"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "\n",
        "    ### GD On Duty: ###\n",
        "    def __init__(self, text, vocab, tokenizer, seq_len=30):\n",
        "    # This function processes a given text into input-target pairs of token sequences.\n",
        "\n",
        "        # Initializes the dataset with the given text, vocabulary, tokenizer, and sequence length\n",
        "        self.data = []\n",
        "        self.vocab = vocab\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        tokens = self.tokenizer(text)\n",
        "        for i in range(len(tokens) - seq_len):\n",
        "            seq = tokens[i:i + seq_len + 1]\n",
        "            # Split each sequence of tokens into two parts: 1) Input Sequence: which includes all tokens except the last one. 2) Target Sequence: which includes all tokens except the first one (shifted by one position).\n",
        "            self.data.append((torch.tensor(self.vocab(seq[:-1]), dtype=torch.long),\n",
        "                              torch.tensor(self.vocab(seq[1:]), dtype=torch.long)))\n",
        "\n",
        "    def __len__(self):\n",
        "    # Returns the length of the dataset\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "    # Returns the specific input-target pair by its position in the dataset.\n",
        "        return self.data[idx]\n",
        "\n",
        "    def tokenize_and_build_vocab(text, tokenizer):\n",
        "    # This function tokenizes a given text, builds a vocabulary including special tokens for unknown words and padding, and ensures that any unknown words will be replaced with a specific token ('<unk>')\n",
        "      tokens = tokenizer(text)\n",
        "      vocab = build_vocab_from_iterator([tokens], specials=['<unk>', '<pad>'])\n",
        "      vocab.set_default_index(vocab['<unk>'])\n",
        "      return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X280CcrcYklK"
      },
      "source": [
        "\n",
        "\n",
        "> The train_model function will:\n",
        "*   Initialize model platform device - Ensure the model is trained on the appropriate hardware for optimal performance and efficiency.\n",
        "*   Split the input data batch into source and target (padding both for equal lengths) - Allows the model learns to predict the next token based on preceding tokens. Padding ensures uniformity in batch processing.\n",
        "*   Load the data and train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iyV5aA943IJO"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataset, vocab, epochs=20, batch_size=32, seq_len=30):\n",
        "\n",
        "    ### GD On Duty: ###\n",
        "    # Initialize the selected device to run the model on\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001) ##EXPERIMENT WITH CHANGING THIS VALUE\n",
        "\n",
        "    def collate_batch(batch):\n",
        "    # This function splits the input batch into source and target, padding both to be of the same length\n",
        "        src_batch, tgt_batch = [], []\n",
        "        for src, tgt in batch:\n",
        "            src_batch.append(src)\n",
        "            tgt_batch.append(tgt)\n",
        "        src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=vocab['<pad>'])\n",
        "        tgt_batch = torch.nn.utils.rnn.pad_sequence(tgt_batch, padding_value=vocab['<pad>'])\n",
        "        return src_batch, tgt_batch\n",
        "\n",
        "    # Load the dataset in batches, shuffle and pad each batch\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, (src, tgt) in enumerate(dataloader):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            src_mask = model.generate_square_subsequent_mask(src.size(0)).to(device)\n",
        "            output = model(src, src_mask)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss:.2f}, Time: {elapsed:.2f}s')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0Leoa6SZTK6"
      },
      "source": [
        "### Finally, putting it all together!\n",
        "\n",
        "\n",
        "> In the code snippet below, we will run the main function which uses the harry potter text data file from Kaggle as input to the Transformer model pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SrrnzTQf3IGl"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    ### GD On Duty: ###\n",
        "    #Use input dataset for modeling as Harry Potter text data from Kaggle (https://www.kaggle.com/datasets/moxxis/harry-potter-lstm?resource=download) - Use Harry_Potter_all_books_preprocessed.txt (5.99 MB)\n",
        "    with open(\"sample_data/harry_potter.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    tokenizer = get_tokenizer('basic_english') #spacy\n",
        "    vocab = TextDataset.tokenize_and_build_vocab(text, tokenizer)\n",
        "    dataset = TextDataset(text, vocab, tokenizer)\n",
        "\n",
        "    ntokens = len(vocab)  # the size of vocabulary\n",
        "    d_model = 512  # smaller embedding dimension\n",
        "    nhid = 2048  # smaller dimension of the feedforward network model in nn.TransformerEncoder\n",
        "    nlayers = 6  # fewer nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "    nhead = 8  # fewer heads in the multiheadattention models\n",
        "    dropout = 0.2  # the dropout value\n",
        "\n",
        "    # ntokens = len(vocab)  # the size of vocabulary\n",
        "    # d_model = 768  # smaller embedding dimension\n",
        "    # nhid = 4096  # smaller dimension of the feedforward network model in nn.TransformerEncoder\n",
        "    # nlayers = 12  # fewer nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "    # nhead = 8  # fewer heads in the multiheadattention models\n",
        "    # dropout = 0.2  # the dropout value\n",
        "\n",
        "    # ntokens = len(vocab)  # the size of vocabulary\n",
        "    # d_model = 512  # embedding dimension\n",
        "    # nhid = 516   #2048  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "    # nlayers = 4   #6  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "    # nhead = 4   #8  # the number of heads in the multiheadattention models\n",
        "    # dropout = 0.2  # the dropout value\n",
        "\n",
        "    #model = TransformerModel(ntokens, d_model, nhead, nhid, nlayers, dropout)\n",
        "\n",
        "    # train_model(model, dataset, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cb_Mamv3Lzf",
        "outputId": "36b8af1a-7543-435a-b9fb-a42e7c9d98b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to sample_data/transformer_model.pth\n"
          ]
        }
      ],
      "source": [
        "def save_model(self, path):\n",
        "  torch.save(self.state_dict(), path)\n",
        "  print(f\"Model saved to {path}\")\n",
        "\n",
        "# Path to save the model\n",
        "model_save_path = \"sample_data/transformer_model_100epochs.pth\"\n",
        "\n",
        "# Save the model\n",
        "TransformerModel.save_model(model, path = model_save_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzZERexVONLw"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "def load_model(path, ntokens, d_model, nhead, nhid, nlayers, dropout=0.2):\n",
        "    \"\"\"\n",
        "    Load a saved model from the specified path.\n",
        "    Args:\n",
        "        path (str): The path to the saved model.\n",
        "        ntokens (int): The size of the vocabulary.\n",
        "        d_model (int): The dimension of the model.\n",
        "        nhead (int): The number of attention heads.\n",
        "        nhid (int): The dimension of the feedforward network.\n",
        "        nlayers (int): The number of transformer encoder layers.\n",
        "        dropout (float): The dropout value.\n",
        "    Returns:\n",
        "        model (nn.Module): The loaded Transformer model.\n",
        "    \"\"\"\n",
        "\n",
        "    model = TransformerModel(ntokens, d_model, nhead, nhid, nlayers, dropout)\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    print(f\"Model loaded from {path}\")\n",
        "    return model\n",
        "\n",
        "ntokens = len(vocab)  # the size of vocabulary\n",
        "d_model = 512  # smaller embedding dimension\n",
        "nhid = 2048  # smaller dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 6  # fewer nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 8  # fewer heads in the multiheadattention models\n",
        "dropout = 0.2  # the dropout value\n",
        "\n",
        "model_save_path = \"transformer_model.pth\"\n",
        "loaded_model = load_model(model_save_path, ntokens, d_model, nhead, nhid, nlayers, dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPMmVM3cZlHW",
        "outputId": "ebd5b6e3-c5b0-4aef-f553-b39d56db3f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "who is harry potter ? had slytherins could were know ! . your forest our at her voice flying in long . a few but rain up a new for on green the rolled on . something thinking your harry world so harry riddle harry was have picked rest highpass before to no and got the whats someone a talk he the told cant be the wasnt publicity of shot . went head chuckled stayed you was harry of students . sending here gaunt that had and the looking he front to room sound more theres hagrid happy a article\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Assuming the necessary classes and functions such as TransformerModel, TextDataset, etc., are already defined.\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "    return mask\n",
        "\n",
        "def generate_text(model, vocab, tokenizer, start_text, max_length=100, temperature=1.0, seq_len=30, device='cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(start_text)\n",
        "    token_ids = [vocab[token] for token in tokens]\n",
        "\n",
        "    src = torch.tensor(token_ids, dtype=torch.long).unsqueeze(1).to(device)\n",
        "    generated_tokens = token_ids.copy()\n",
        "\n",
        "    for _ in range(max_length - len(tokens)):\n",
        "        with torch.no_grad():\n",
        "            mask = generate_square_subsequent_mask(src.size(0)).to(device)\n",
        "            output = model(src, mask)\n",
        "\n",
        "        next_token_logits = output[-1, 0, :] / temperature\n",
        "        next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
        "        next_token_id = torch.multinomial(next_token_probs, num_samples=1).item()\n",
        "\n",
        "        generated_tokens.append(next_token_id)\n",
        "        src = torch.tensor(generated_tokens[-seq_len:], dtype=torch.long).unsqueeze(1).to(device)\n",
        "    itos = vocab.get_itos()\n",
        "    generated_text = ' '.join([itos[token_id] for token_id in generated_tokens])\n",
        "    return generated_text.strip()\n",
        "\n",
        "# Example usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "start_text = \"Who is Harry Potter?\"\n",
        "generated_text = generate_text(model, vocab, tokenizer, start_text, max_length=100, device=device)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdG85pl4AZR5",
        "outputId": "11827d0b-7754-444b-f9f9-2e621e93ab54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab()\n",
            "<function _basic_english_normalize at 0x7fd36306b910>\n",
            "Who is Harry Potter?\n"
          ]
        }
      ],
      "source": [
        "print(vocab)\n",
        "print(tokenizer)\n",
        "print(start_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "TAmiqI3VZlDX",
        "outputId": "cce01677-8f96-47b0-d74e-c07535009e22"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'generate_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6aa54048067c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test the model by generating text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Tell me a little bit about Harry Potter?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_text' is not defined"
          ]
        }
      ],
      "source": [
        "# Test the model by generating text\n",
        "start_text = \"Tell me a little bit about Harry Potter?\"\n",
        "generated_text = generate_text(model, vocab, tokenizer, start_text, max_length=100, temperature = 1.0, device = 'cpu')\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "erWqU3sf3H7z",
        "outputId": "90ec9e24-074b-40e2-f9c4-9ae4baa16ec3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tell me a little bit about Harry Potter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model, vocab, tokenizer, start_text, max_length=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Romyg_g50O_l"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDxKwIV-0POk"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}